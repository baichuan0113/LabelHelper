{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator,\n",
    "    OpenAIModel,\n",
    "    QAEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    run_evals,\n",
    ")\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.close_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(query):\n",
    "    similar_response = db.similarity_search(query, k=3)\n",
    "    page_contents_array = [doc.page_content for doc in similar_response]\n",
    "    return page_contents_array\n",
    "\n",
    "def generate_response(message):\n",
    "    best_practice = retrieve_info(message)\n",
    "    response = chain.run(message=message, best_practice=best_practice)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"response.csv\", encoding='iso-8859-1')\n",
    "documents = loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo-2024-04-09\")\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Options include \"stuff\", \"refine\", \"map_reduce\", \"map_rerank\"\n",
    "    retriever=db.as_retriever(),\n",
    "    metadata={\"application_type\": \"question_answering\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [opentelemetry.instrumentation.instrumentor] Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file_path = \"test_queries.json\"\n",
    "with open(\"test_queries.json\", 'r') as file:\n",
    "    test_queries = json.load(file)\n",
    "queries = [item['query'] for item in test_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]ERROR [asyncio] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-5' coro=<BulkInserter._bulk_insert() running at /Users/baichuan/.pyenv/versions/3.11.9/envs/finalenv/lib/python3.11/site-packages/phoenix/db/bulk_inserter.py:103> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "ERROR [asyncio] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-2683' coro=<BulkInserter._bulk_insert() running at /Users/baichuan/.pyenv/versions/3.11.9/envs/finalenv/lib/python3.11/site-packages/phoenix/db/bulk_inserter.py:103> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "ERROR [asyncio] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-2690' coro=<BulkInserter._bulk_insert() running at /Users/baichuan/.pyenv/versions/3.11.9/envs/finalenv/lib/python3.11/site-packages/phoenix/db/bulk_inserter.py:103> wait_for=<Future finished result=None>>\n",
      "ERROR [asyncio] Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-3389' coro=<BulkInserter._bulk_insert() running at /Users/baichuan/.pyenv/versions/3.11.9/envs/finalenv/lib/python3.11/site-packages/phoenix/db/bulk_inserter.py:103> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How do I get started with Roboflow‚Äôs Outsource Labeling service?', 'result': \"I don't know the specific steps to get started with Roboflow‚Äôs Outsource Labeling service. You might want to visit the Roboflow website or contact their customer support for detailed guidance on how to access and use the Outsource Labeling service.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for query in tqdm(queries[5:6]):\n",
    "    res = chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No retrieval documents found.\n"
     ]
    }
   ],
   "source": [
    "queries_df = get_qa_with_reference(px.Client())\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Empty DataFrame\n",
      "Columns: [reference, document_score, context.trace_id, input]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(queries_df)\n",
    "print(retrieved_documents_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace_df = px.Client().get_spans_dataframe()\n",
    "# trace_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 (100.0%) | ‚è≥ 00:11<00:00 |  1.09it/s\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 (100.0%) | ‚è≥ 00:11<00:00 |  2.10it/s\n"
     ]
    }
   ],
   "source": [
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    "    DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
